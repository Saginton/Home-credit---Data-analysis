{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score \n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cbt\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_table_dtypes(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    # implement here all desired dtypes for tables\n",
    "    # the following is just an example\n",
    "    for col in df.columns:\n",
    "        # last letter of column name will help you determine the type\n",
    "        if col[-1] in (\"P\", \"A\"):\n",
    "            df = df.with_columns(pl.col(col).cast(pl.Float64).alias(col))\n",
    "\n",
    "    return df\n",
    "\n",
    "def convert_strings(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for col in df.columns:  \n",
    "        if df[col].dtype.name in ['object', 'string']:\n",
    "            df[col] = df[col].astype(\"string\").astype('category')\n",
    "            current_categories = df[col].cat.categories\n",
    "            new_categories = current_categories.to_list() + [\"Unknown\"]\n",
    "            new_dtype = pd.CategoricalDtype(categories=new_categories, ordered=True)\n",
    "            df[col] = df[col].astype(new_dtype)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_basetable = pl.read_csv(\"csv_files/train/train_base.csv\")\n",
    "train_static = pl.concat(\n",
    "    [\n",
    "        pl.read_csv(\"csv_files/train/train_static_0_0.csv\").pipe(set_table_dtypes),\n",
    "        pl.read_csv(\"csv_files/train/train_static_0_1.csv\").pipe(set_table_dtypes),\n",
    "    ],\n",
    "    how=\"vertical_relaxed\",\n",
    ")\n",
    "train_static_cb = pl.read_csv(\"csv_files/train/train_static_cb_0.csv\").pipe(set_table_dtypes)\n",
    "train_person_1 = pl.read_csv(\"csv_files/train/train_person_1.csv\").pipe(set_table_dtypes) \n",
    "train_credit_bureau_b_2 = pl.read_csv(\"csv_files/train/train_credit_bureau_b_2.csv\").pipe(set_table_dtypes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_basetable = pl.read_csv(\"csv_files/test/test_base.csv\")\n",
    "test_static = pl.concat(\n",
    "    [\n",
    "        pl.read_csv(\"csv_files/test/test_static_0_0.csv\").pipe(set_table_dtypes),\n",
    "        pl.read_csv(\"csv_files/test/test_static_0_1.csv\").pipe(set_table_dtypes),\n",
    "        pl.read_csv(\"csv_files/test/test_static_0_2.csv\").pipe(set_table_dtypes),\n",
    "    ],\n",
    "    how=\"vertical_relaxed\",\n",
    ")\n",
    "test_static_cb = pl.read_csv(\"csv_files/test/test_static_cb_0.csv\").pipe(set_table_dtypes)\n",
    "test_person_1 = pl.read_csv(\"csv_files/test/test_person_1.csv\").pipe(set_table_dtypes) \n",
    "test_credit_bureau_b_2 = pl.read_csv(\"csv_files/test/test_credit_bureau_b_2.csv\").pipe(set_table_dtypes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Polars DataFrames to Pandas for easier manipulation in later stages (optional)\n",
    "train_basetable = train_basetable.to_pandas()\n",
    "train_static = train_static.to_pandas()\n",
    "train_static_cb = train_static_cb.to_pandas()\n",
    "train_person_1 = train_person_1.to_pandas()\n",
    "test_basetable = test_basetable.to_pandas()\n",
    "test_static = test_static.to_pandas()\n",
    "test_static_cb = test_static_cb.to_pandas()\n",
    "test_person_1 = test_person_1.to_pandas()\n",
    "\n",
    "# Example of a simple feature engineering function\n",
    "def create_features(base_table, static, static_cb, person_1):\n",
    "    # Join your tables here. Example:\n",
    "    df = pd.merge(base_table, static, on=\"case_id\", how=\"left\")\n",
    "    df = pd.merge(df, static_cb, on=\"case_id\", how=\"left\")\n",
    "    df = pd.merge(df, person_1, on=\"case_id\", how=\"left\")\n",
    "    \n",
    "    # Assuming `df` is your DataFrame after conversion to Pandas\n",
    "    df['annuity_income_ratio'] = df['annuity_780A'] / df['maininc_215A']\n",
    "    df['avg_outstanding_balance_income_ratio'] = df['avgoutstandbalancel6m_4187114A'] / df['maininc_215A']\n",
    "    df['payment_income_ratio_12m'] = df['avgpmtlast12m_4525200A'] / df['maininc_215A']\n",
    "    df['debt_credit_ratio'] = df['totaldebt_9A'] / df['credamount_770A']\n",
    "    \n",
    "    df['bankacctype_710L'] = df['bankacctype_710L'].astype('category')\n",
    "    # Assuming 'df' is your DataFrame\n",
    "    # Select only numeric columns for median calculation\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "    # Calculate the median only for numeric columns\n",
    "    numeric_medians = numeric_df.median()\n",
    "\n",
    "    # Fill missing values in numeric columns with their respective medians\n",
    "    df.fillna(numeric_medians, inplace=True)\n",
    "        \n",
    "    # Convert categorical variables to type 'category' for LightGBM, XGBoost, and CatBoost\n",
    "    df = convert_strings(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = create_features(train_basetable, train_static, train_static_cb, train_person_1)\n",
    "test_df = create_features(test_basetable, test_static, test_static_cb, test_person_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 4104.14 MB\n",
      "Memory usage after optimization is: 1602.59 MB\n",
      "Decreased by 61.0%\n",
      "[LightGBM] [Info] Number of positive: 79567, number of negative: 2299625\n",
      "[LightGBM] [Info] Total Bins 30567\n",
      "[LightGBM] [Info] Number of data points in the train set: 2379192, number of used features: 252\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.033443 -> initscore=-3.363902\n",
      "[LightGBM] [Info] Start training from score -3.363902\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5]\tvalid_0's auc: 0.727821\tvalid_0's binary_logloss: 0.141313\n",
      "[0]\tvalidation_0-logloss:0.20140\n",
      "[1]\tvalidation_0-logloss:0.19703\n",
      "[2]\tvalidation_0-logloss:0.19294\n",
      "[3]\tvalidation_0-logloss:0.18910\n",
      "[4]\tvalidation_0-logloss:0.18550\n",
      "date_decision                  0\n",
      "bankacctype_710L         2114985\n",
      "cardtype_51L             2523887\n",
      "clientscnt_136L          2972123\n",
      "credtype_322L                  3\n",
      "                          ...   \n",
      "role_1084L                  6117\n",
      "role_993L                2949075\n",
      "safeguarantyflag_411L    1447334\n",
      "sex_738L                 1447332\n",
      "type_25L                    6117\n",
      "Length: 103, dtype: int64\n",
      "date_decision            category\n",
      "bankacctype_710L         category\n",
      "cardtype_51L             category\n",
      "clientscnt_136L          category\n",
      "credtype_322L            category\n",
      "                           ...   \n",
      "role_1084L               category\n",
      "role_993L                category\n",
      "safeguarantyflag_411L    category\n",
      "sex_738L                 category\n",
      "type_25L                 category\n",
      "Length: 103, dtype: object\n",
      "date_decision            0\n",
      "bankacctype_710L         0\n",
      "cardtype_51L             0\n",
      "clientscnt_136L          0\n",
      "credtype_322L            0\n",
      "                        ..\n",
      "role_1084L               0\n",
      "role_993L                0\n",
      "safeguarantyflag_411L    0\n",
      "sex_738L                 0\n",
      "type_25L                 0\n",
      "Length: 103, dtype: int64\n",
      "date_decision            object\n",
      "bankacctype_710L         object\n",
      "cardtype_51L             object\n",
      "clientscnt_136L          object\n",
      "credtype_322L            object\n",
      "                          ...  \n",
      "role_1084L               object\n",
      "role_993L                object\n",
      "safeguarantyflag_411L    object\n",
      "sex_738L                 object\n",
      "type_25L                 object\n",
      "Length: 103, dtype: object\n",
      "0:\tlearn: 0.6170569\ttest: 0.6170420\tbest: 0.6170420 (0)\ttotal: 4.36s\tremaining: 17.4s\n",
      "4:\tlearn: 0.4055700\ttest: 0.4397578\tbest: 0.4397578 (4)\ttotal: 13.7s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.4397578154\n",
      "bestIteration = 4\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x1a5939bd3d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(f'Memory usage of dataframe is {start_mem:.2f} MB')\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type not in ['object', 'category']:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            # Skip min/max operations for categorical or object columns\n",
    "            continue  # You can also convert to ordered categorical here if applicable\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(f'Memory usage after optimization is: {end_mem:.2f} MB')\n",
    "    print(f'Decreased by {(100 * (start_mem - end_mem) / start_mem):.1f}%')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = reduce_mem_usage(train_df)\n",
    "train_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "for col in train_df.select_dtypes(include=['float16', 'float32', 'float64']).columns:\n",
    "    train_df[col] = train_df[col].fillna(train_df[col].median())\n",
    "\n",
    "# Now proceed with your train-test split and model training as before\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_df.drop(['target', 'case_id'], axis=1),\n",
    "    train_df['target'], test_size=0.2, random_state=42\n",
    ")\n",
    "X_test = test_df.drop(['case_id'], axis=1)\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    n_estimators=5,\n",
    "    learning_rate=0.05,\n",
    "    random_state=42,\n",
    "    force_col_wise=True\n",
    ")\n",
    "\n",
    "# Fit the LightGBM model with early stopping\n",
    "lgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric='auc',\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=True)]  # This is now correctly placed within fit_params\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=5, \n",
    "    learning_rate=0.05, \n",
    "    random_state=42, \n",
    "    use_label_encoder=False, \n",
    "    eval_metric='logloss',\n",
    "    early_stopping_rounds=50,  # Place early_stopping_rounds here\n",
    "    enable_categorical=True  # Ensure your XGBoost version supports this\n",
    ")\n",
    "\n",
    "# Fit the XGBoost model with early stopping\n",
    "xgb_model.fit(\n",
    "    X_train, y_train, \n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=True\n",
    ")\n",
    "cat_features = ['date_decision', 'bankacctype_710L','cardtype_51L','clientscnt_136L', 'credtype_322L', 'datefirstoffer_1144D', 'datelastinstal40dpd_247D', 'datelastunpaid_3546854D', 'deferredmnthsnum_166L', 'disbursementtype_67L', 'dtlastpmtallstes_4499206D', 'equalitydataagreement_891L', 'equalityempfrom_62L', 'firstclxcampaign_1125D', 'firstdatedue_489D', 'inittransactioncode_186L', 'interestrategrace_34L', 'isbidproductrequest_292L', 'isdebitcard_729L', 'lastactivateddate_801D', 'lastapplicationdate_877D', 'lastapprcommoditycat_1041M', 'lastapprcommoditytypec_5251766M', 'lastapprdate_640D', 'lastcancelreason_561M', 'lastdelinqdate_224D', 'lastdependentsnum_448L', 'lastrejectcommoditycat_161M', 'lastrejectcommodtypec_5251769M', 'lastrejectdate_50D', 'lastrejectreason_759M', 'lastrejectreasonclient_4145040M', 'lastrepayingdate_696D', 'lastst_736L', 'maxdpdinstldate_3546855D', 'numinstlswithdpd5_4187116L', 'numinstmatpaidtearly2d_4499204L', 'numinstpaid_4499208L', 'numinstpaidearly3dest_4493216L', 'numinstpaidearly5dest_4493211L', 'numinstpaidearly5dobd_4499205L', 'numinstpaidearlyest_4493214L', 'numinstpaidlastcontr_4325080L', 'numinstregularpaidest_4493210L', 'numinsttopaygrest_4493213L', 'numinstunpaidmaxest_4493212L', 'opencred_647L', 'paytype1st_925L', 'paytype_783L', 'payvacationpostpone_4187118D', 'previouscontdistrict_112M', 'twobodfilling_608L', 'typesuite_864L', 'validfrom_1069D', 'assignmentdate_238D', 'assignmentdate_4527235D', 'assignmentdate_4955616D', 'birthdate_574D', 'contractssum_5085716L', 'dateofbirth_337D', 'dateofbirth_342D', 'description_5085714M', 'education_1103M', 'education_88M', 'maritalst_385M', 'maritalst_893M', 'pmtcount_4527229L', 'pmtcount_4955617L', 'requesttype_4525192L', 'responsedate_1012D', 'responsedate_4527233D', 'responsedate_4917613D', 'riskassesment_302T', 'birth_259D', 'birthdate_87D', 'contaddr_district_15M', 'contaddr_matchlist_1032L', 'contaddr_smempladdr_334L', 'contaddr_zipcode_807M', 'education_927M', 'empl_employedfrom_271D', 'empl_employedtotal_800L', 'empl_industry_691L', 'empladdr_district_926M', 'empladdr_zipcode_114M', 'familystate_447L', 'gender_992L', 'housetype_905L', 'housingtype_772L', 'incometype_1044T', 'isreference_387L', 'language1_981M', 'maritalst_703L', 'registaddr_district_1083M', 'registaddr_zipcode_184M', 'relationshiptoclient_415T', 'relationshiptoclient_642T', 'remitter_829L', 'role_1084L', 'role_993L', 'safeguarantyflag_411L', 'sex_738L', 'type_25L']\n",
    "print(train_df[cat_features].isna().sum())  # Check for remaining NaN values\n",
    "print(train_df[cat_features].dtypes) \n",
    "\n",
    "for col in cat_features:\n",
    "    # For the training set\n",
    "    train_df[col] = train_df[col].astype(str).replace('nan', 'missing').fillna('missing')\n",
    "    X_val[col] = X_val[col].astype(str).replace('nan', 'missing').fillna('missing')\n",
    "for col in cat_features:\n",
    "    X_train[col] = X_train[col].astype(str).fillna('missing')\n",
    "    X_val[col] = X_val[col].astype(str).fillna('missing')    \n",
    "\n",
    "print(train_df[cat_features].isna().sum())  # Check for remaining NaN values\n",
    "print(train_df[cat_features].dtypes) \n",
    "\n",
    "    \n",
    "\n",
    "cbt_model = cbt.CatBoostClassifier(\n",
    "    iterations=5, \n",
    "    learning_rate=0.05, \n",
    "    random_state=42, \n",
    "    verbose=100,\n",
    "    early_stopping_rounds=50,\n",
    "    cat_features = cat_features\n",
    ")\n",
    "# Fit the CatBoost model\n",
    "cbt_model.fit(\n",
    "    X_train, y_train, \n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   case_id  score\n",
      "0    57543    0.5\n",
      "1    57549    0.5\n",
      "2    57551    0.5\n",
      "3    57552    0.5\n",
      "4    57569    0.5\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
